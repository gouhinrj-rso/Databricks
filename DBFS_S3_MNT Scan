# Databricks | Python
# Purpose: enumerate table storage locations (S3/ADLS/DBFS) via DESCRIBE DETAIL
# Hint: flip FULL_INVENTORY to scan all catalogs/schemas; otherwise target one schema.

from pyspark.sql import Row
from pyspark.sql import functions as F

# --------- CONFIG (edit these) ---------
FULL_INVENTORY = False                     # True = scan all catalogs/schemas; False = scan TARGET_CATALOG/TARGET_SCHEMA only
TARGET_CATALOG = None                      # e.g., "main" or leave None if you use hive_metastore
TARGET_SCHEMA  = "bld_a4l_restricted_analytics"   # <- your schema
WRITE_CSV_PATH = "/dbfs/FileStore/tables/table_locations.csv"    # output (optional)
# ---------------------------------------

def describe_detail_robust(fully_qualified_name: str) -> Row:
    """
    Try DESCRIBE DETAIL; if table is a view or fails, return minimal row with error.
    """
    try:
        dd = spark.sql(f"DESCRIBE DETAIL {fully_qualified_name}")
        r = dd.limit(1).collect()[0].asDict()
        return Row(
            catalog=r.get("catalogName"),
            schema=r.get("schemaName"),
            table=r.get("name"),
            table_type=r.get("type"),
            format=r.get("format"),
            location=r.get("location"),
            numFiles=r.get("numFiles"),
            sizeInBytes=r.get("sizeInBytes"),
            properties=str(r.get("properties")) if r.get("properties") is not None else None,
            comment=r.get("description"),
            error=None
        )
    except Exception as e:
        # Views or permission issues typically land here
        try:
            parts = fully_qualified_name.split(".")
            cat, sch, tbl = (parts + [None, None, None])[:3]
        except:
            cat, sch, tbl = (None, None, fully_qualified_name)
        return Row(
            catalog=cat, schema=sch, table=tbl, table_type=None, format=None,
            location=None, numFiles=None, sizeInBytes=None, properties=None, comment=None,
            error=str(e)[:500]
        )

rows = []

if FULL_INVENTORY:
    # Deep scan: all catalogs → schemas → tables
    catalogs = [r.catalog for r in spark.sql("SHOW CATALOGS").collect()]
    for cat in catalogs:
        schemas = [r.namespace for r in spark.sql(f"SHOW SCHEMAS IN {cat}").collect()]
        for sch in schemas:
            tables = [r.tableName for r in spark.sql(f"SHOW TABLES IN {cat}.{sch}").collect()]
            for tbl in tables:
                fqn = f"{cat}.{sch}.{tbl}"
                rows.append(describe_detail_robust(fqn))
else:
    # Quick scan: target a single schema (works for Unity Catalog or hive_metastore)
    if TARGET_CATALOG:
        fqn_schema = f"{TARGET_CATALOG}.{TARGET_SCHEMA}"
    else:
        # hive_metastore or default catalog
        fqn_schema = TARGET_SCHEMA
    tables = [r.tableName for r in spark.sql(f"SHOW TABLES IN {fqn_schema}").collect()]
    for tbl in tables:
        fqn = f"{fqn_schema}.{tbl}"
        rows.append(describe_detail_robust(fqn))

df = spark.createDataFrame(rows) \
    .withColumn("sizeMB", (F.col("sizeInBytes")/F.lit(1024*1024)).cast("double")) \
    .select(
        "catalog","schema","table","table_type","format",
        "location","numFiles","sizeInBytes","sizeMB","comment","properties","error"
    ) \
    .orderBy("catalog","schema","table")

display(df)

# Optional: write a CSV you can download (DBFS path -> visible in Data/Files UI)
try:
    df.toPandas().to_csv(WRITE_CSV_PATH, index=False)
    print(f"Saved: {WRITE_CSV_PATH}")
except Exception as e:
    print(f"CSV save skipped: {e}")
